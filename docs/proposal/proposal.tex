\documentclass[journal]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Intelligent Sprint Analysis Using Agentic System for Startup Projects}

\author{
\small
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccc@{}}
\begin{tabular}{@{}c@{}}
\textbf{Bibek Gupta} \\
\scriptsize
\textit{Department of Computer Science} \\
\textit{Florida Polytechnic University} \\
Lakeland, Florida, USA \\
bgupta2957@floridapoly.edu
\end{tabular}
&
\begin{tabular}{@{}c@{}}
\textbf{Saarupya Sunkara} \\
\scriptsize
\textit{Department of Computer Science} \\
\textit{Florida Polytechnic University} \\
Lakeland, Florida, USA \\
vsunkara3613@floridapoly.edu
\end{tabular}
&
\begin{tabular}{@{}c@{}}
\textbf{Siwani Sah} \\
\scriptsize
\textit{Department of Computer Science} \\
\textit{Florida Polytechnic University} \\
Lakeland, Florida, USA \\
ssah2942@floridapoly.edu
\end{tabular}
&
\begin{tabular}{@{}c@{}}
\textbf{Deepthi Reddy Chelladi } \\
\scriptsize
\textit{Department of Computer Science} \\
\textit{Florida Polytechnic University} \\
Lakeland, Florida, USA \\
dchelladi3522@floridapoly.edu
\end{tabular}
\end{tabular*}
}

\maketitle

\begin{abstract}
Sprint management in small startup teams faces critical challenges due to limited resources, absence of dedicated project managers, and manual tracking overhead consuming 30-40\% of technical leads' time. This project proposes an intelligent sprint analysis system leveraging multi-agent large language models with retrieval-augmented generation to provide real-time, explainable sprint health assessments for startups managing 2-3 GitHub repositories. The system integrates six data modalities—code changes, textual communications, temporal patterns, dependency graphs, sentiment analysis, and CI/CD metrics—processed through specialized agents for pattern recognition, blocker detection, and recommendation generation. Using GitHub Archive data comprising 9,000 sprint instances from 500 startup organizations combined with 5,000 synthetically generated scenarios, we employ parameter-efficient fine-tuning with LoRA adapters enabling deployment on standard developer laptops without cloud infrastructure. The evaluation framework compares our agentic approach against rule-based heuristics, gradient boosting baselines, and single-prompt LLM systems across predictive accuracy, explanation quality, and operational feasibility metrics. Target outcomes include sprint outcome prediction achieving F1-score $\geq$0.85, blocker detection at $\geq$0.88 F1-score, and evidence-based explanations targeting stakeholder trust scores $\geq$4.2/5.0 (compared to baseline 2.8/5.0 without RAG), while maintaining sub-60-second p95 latency on 16GB RAM laptops.
\end{abstract}

\section{Team Information}

\textbf{Project Title:} Intelligent Sprint Analysis Using Agentic System for Startup Projects

\noindent\textbf{Team Members and Roles:}
\begin{itemize}
    \item \textbf{Bibek} -- Project Lead, LLM Integration, Multi-Agent Architecture
    \item \textbf{Saarupya} -- Data Engineering, GitHub API Integration, Feature Extraction
    \item \textbf{Siwani} -- Machine Learning Models, Evaluation Framework, Statistical Analysis
    \item \textbf{Deepthi} -- Frontend Development, Visualization Dashboard, User Experience
\end{itemize}

\section{Background and Significance}

\subsection{Introduction}

Software development in startup environments operates under severe resource constraints where small teams of 3-10 developers manage 2-3 tightly coupled repositories without dedicated project management staff. Research indicates that 82\% of early-stage startups lack formal project managers, forcing engineering leads to manually track sprint progress a task consuming 6-10 hours weekly that could otherwise contribute to product development \cite{kalliamvakou2014promises}. Traditional sprint estimation techniques rely heavily on historical velocity data and expert judgment \cite{usman2014effort}, making them ineffective for resource-constrained startups lacking comprehensive project histories. GitHub has emerged as the dominant platform for version control and collaboration, hosting over 100 million repositories and generating millions of development events daily. However, existing project management tools designed for enterprise environments require extensive configuration, historical data accumulation periods of 6-12 months, and ongoing subscription costs of \$500-2000 monthly barriers prohibitive for resource-constrained startups.

\begin{figure*}[htbp]
\centerline{\includegraphics[width=0.85\textwidth]{figures/5_end_to_end_workflow.png}}
\caption{End-to-End System Workflow from GitHub data ingestion to final output generation. The workflow consists of seven stages: (1) Input from GitHub repositories, (2) Data Collection via GitHub API, (3) Feature Engineering producing 120-dimensional vectors, (4) Multi-Agent Processing, (5) RAG Enhancement with historical context, (6) LLM Processing using LoRA-fine-tuned Llama 2, and (7) Output generation including predictions, alerts, recommendations, and explanations.}
\label{fig:workflow}
\end{figure*}

Recent advances in large language models, particularly instruction-tuned variants like Llama-3 and GPT-4, demonstrate remarkable capabilities in understanding complex contexts, reasoning about scenarios, and generating actionable insights \cite{touvron2023llama}. Despite these capabilities, their application to cross-repository project management remains largely unexplored. Most existing research focuses on single-repository code generation or isolated analyses, missing the interconnected nature of startup development where delays in one repository cascade to dependent projects. Figure~\ref{fig:workflow} illustrates our proposed end-to-end system addressing these challenges.

\subsection{Motivation}

The motivation for this research emerges from three critical gaps in current practice:

\textbf{Gap 1: Over-engineered Tools for Small Teams.} Platforms like Jira, Azure DevOps, and Monday.com target enterprise customers requiring weeks of setup, dedicated administrators, and substantial historical datasets before delivering value. Small startups generating 50-200 GitHub events daily across 2-3 repositories need lightweight solutions providing immediate insights without lengthy onboarding periods or expensive infrastructure.

\textbf{Gap 2: Limited Cross-Repository Intelligence.} Existing analytics tools analyze each repository independently despite evidence showing 34\% of startup sprint delays stem from untracked cross-repository dependencies \cite{bird2009putting}. When a backend API change breaks frontend integration tests, or a shared library update introduces breaking changes, traditional single-repository monitors fail to detect these propagating risks until manual discovery occurs.

\textbf{Gap 3: Lack of Explainable AI for Project Management.} While machine learning models achieve reasonable accuracy predicting project outcomes \cite{choetkiertikul2018predicting}, they typically function as black boxes generating predictions without actionable explanations. Startup technical leads require understanding \textit{why} a sprint appears at risk and \textit{what specific actions} can mitigate identified issues. Without transparency, even accurate predictions receive low adoption due to trust deficits.

\subsection{Problem Statement}

This project addresses the following problem: \textit{How can we develop a lightweight, instantly deployable intelligent system providing real-time, explainable sprint insights for small startup teams managing 2-3 repositories without requiring extensive historical data, complex configuration, or cloud infrastructure dependencies?}

Specific challenges include:
\begin{itemize}
    \item Extracting and fusing multi-modal development signals (code, text, temporal patterns, dependencies, sentiment, CI/CD) with minimal computational overhead
    \item Enabling real-time analysis with sub-60-second latency on consumer-grade hardware (16GB RAM laptops)
    \item Providing explainable predictions with evidence attribution building stakeholder trust
    \item Supporting instant deployment using synthetic data when organizational history is unavailable
    \item Tracking cross-repository dependencies effectively within small repository sets
\end{itemize}

\subsection{Related Work}

Our literature analysis examined recent advances across three domains:

\textbf{GitHub Repository Analytics.} Kalliamvakou et al. \cite{kalliamvakou2014promises} established foundational understanding of GitHub as a research platform, identifying key metrics like commit frequency, issue closure rates, and pull request merge times. Bird et al. \cite{bird2009putting} demonstrated that socio-technical network analysis predicts failures with 74-78\% accuracy, though their single-repository focus limits applicability to multi-repo startup contexts.

\textbf{LLM Applications in Software Engineering.} Touvron et al. \cite{touvron2023llama} introduced Llama-3, showing instruction-tuned models achieve strong performance on code understanding benchmarks. Wang et al. \cite{wang2024survey} provide comprehensive analysis of large language models for code generation, demonstrating their capabilities in understanding programming semantics and generating contextually appropriate solutions. Pre-trained models like CodeBERT \cite{feng2020codebert} learn joint representations of code and natural language, enabling semantic understanding of developer communications and code changes. Recent work explores LLMs for code generation, bug detection, and documentation synthesis, yet project-level sprint management applications remain underexplored. Most studies evaluate cloud APIs rather than locally deployable models suitable for privacy-sensitive startup codebases.

\textbf{Sprint and Milestone Prediction.} Choetkiertikul et al. \cite{choetkiertikul2018predicting} achieved 75-87\% accuracy predicting sprint success using LSTM networks and gradient boosting on velocity metrics and temporal patterns. However, these approaches employ handcrafted features requiring domain expertise, lack semantic understanding of code changes and communications, and provide no mechanisms for explainability or evidence attribution. Ribeiro et al. \cite{ribeiro2016should} emphasize the critical importance of model interpretability in machine learning systems, arguing that prediction explanations are essential for building user trust and enabling actionable insights a principle particularly vital for project management contexts where stakeholders must understand reasoning behind risk assessments.

\textbf{Retrieval-Augmented Generation.} Lewis et al. \cite{lewis2020retrieval} demonstrated RAG improves factual accuracy and reduces hallucination by retrieving relevant context before generation. Applications span question answering and summarization, but software project management contexts remain unexplored. We hypothesize RAG enables citing specific past sprint examples as evidence supporting predictions, thereby increasing trust.

\subsection{Innovation and Contributions}

This project introduces four novel contributions:

\textbf{Innovation 1: Multi-Modal Multi-Agent Architecture.} We present the first system integrating six data modalities code diffs, issue semantics, temporal burndown, dependency graphs, communication sentiment, and CI/CD metrics processed through specialized LLM agents orchestrated via LangGraph. Each agent focuses on specific analysis aspects (feature extraction, pattern recognition, risk assessment, recommendation generation, explanation synthesis) enabling deeper reasoning than monolithic models.

\textbf{Innovation 2: Cross-Repository Dependency Intelligence.} Our system automatically constructs dependency graphs across repositories by analyzing code imports, issue references, and shared contributor patterns. Using graph neural network embeddings combined with LLM reasoning, we predict how delays propagate between dependent repositories addressing the 34\% of failures current tools miss. Figure~\ref{fig:cross_repo} illustrates how the system tracks dependencies across Backend, Frontend, and Shared Library repositories to detect and predict cascading risks.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/4_cross_repo_dependencies.png}}
\caption{Cross-Repository Dependency Intelligence in a small startup setting. The system tracks dependencies between Backend, Frontend, and Shared Libraries to predict cascading risks and prevent sprint delays.}
\label{fig:cross_repo}
\end{figure}

\textbf{Innovation 3: RAG-Enhanced Explainability.} Unlike black-box predictors, our explainer agent retrieves similar historical sprint cases from a ChromaDB vector store and generates natural language explanations citing specific commits, issues, and pull requests as evidence. This evidence attribution mechanism increases transparency and enables stakeholder verification of AI reasoning. As shown in Figure~\ref{fig:rag_pipeline}, the RAG pipeline embeds the current sprint context, retrieves top-5 similar historical cases, and injects this augmented context into the LLM for evidence-backed explanation generation.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/2_rag_pipeline.png}}
\caption{RAG Pipeline Architecture for Explainability. The system embeds the current sprint context, retrieves top-5 similar historical cases from ChromaDB, and injects this augmented context into Llama 2 to generate evidence-backed explanations with historical references.}
\label{fig:rag_pipeline}
\end{figure}

\textbf{Innovation 4: Synthetic Data Bootstrapping.} To enable instant deployment in new organizations lacking historical data, we employ LLM-based synthetic sprint scenario generation creating realistic development patterns across success, delay, and failure outcomes. Parameter-efficient fine-tuning using LoRA adapters combines synthetic scenarios with minimal real data, reducing cold-start periods from 6-12 months to under one week.

Figure~\ref{fig:comparison} presents a comprehensive comparison of our proposed approach against alternative solutions. Our Local LLM + RAG system achieves near-GPT-4 accuracy (93\%) while maintaining full privacy and zero operational cost, demonstrating clear advantages over both cloud-based and vanilla local LLM approaches.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{figures/3_approach_comparison.png}}
\caption{Approach Comparison Matrix comparing Cloud LLM (GPT-4), Local LLM (Vanilla), and our proposed Local LLM + RAG approach across four key criteria: Accuracy, Privacy, Cost, and Explainability. The proposed approach achieves near-GPT-4 accuracy (93\%) while maintaining full privacy and zero cost.}
\label{fig:comparison}
\end{figure}

\section{Proposed Methods}

\subsection{Materials: Dataset}

We construct a comprehensive multi-modal dataset from two sources:

\subsubsection{Primary Dataset: GitHub Archive}

\textbf{Source:} GitHub Archive (\url{https://www.gharchive.org/}) provides hourly event streams capturing all public GitHub activity.

\textbf{Collection Period:} March 2023 - February 2026 (36 months)

\textbf{Selection Criteria:} Organizations meeting startup profile requirements: (1) 2-3 core repositories containing $\geq$80\% of development activity, (2) 3-10 unique contributors, (3) sustained activity $\geq$20 events weekly, (4) visible milestone or project board tracking, (5) active issue management and pull request workflows.

\textbf{Dataset Size:}
\begin{itemize}
    \item Organizations: 500 qualifying startups
    \item Repositories: 1,500 total (3 per organization average)
    \item Sprint instances: 9,000 (training: 6,000; validation: 2,000; test: 1,000)
    \item Events captured: $\sim$4.5 million (commits, issues, PRs, comments, reviews, CI runs)
\end{itemize}

Figure~\ref{fig:dataset} illustrates the composition of our dataset, combining real and synthetic data with a temporal split strategy to ensure robust evaluation.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/5_dataset_composition.png}}
\caption{Dataset Composition and Split Strategy. The dataset combines 9,000 real sprint instances from GitHub Archive with 5,000 synthetic scenarios, split temporally to prevent data leakage and ensure robust evaluation.}
\label{fig:dataset}
\end{figure}

\textbf{Features per Sprint:} Each sprint instance contains:
\begin{itemize}
    \item \textit{Code activity:} Commits (authors, timestamps, diffs, complexity metrics), files changed, lines added/deleted
    \item \textit{Issue tracking:} Issue creation/closure events, labels, assignments, state transitions, comment threads
    \item \textit{Pull requests:} PR lifecycle events, review comments, approvals, merge status, review response times
    \item \textit{CI/CD:} Pipeline runs, test results, build status, deployment events
    \item \textit{Dependencies:} Intra-repository module dependencies, cross-repository import relationships, shared contributor networks
    \item \textit{Temporal:} Event timestamps enabling burndown calculation, velocity tracking, activity rhythm analysis
\end{itemize}

\textbf{Labels:} Ground truth labels assigned through algorithmic extraction from GitHub APIs:
\begin{itemize}
    \item \textit{Sprint outcome} (3-class): SUCCESS ($\geq$90\% planned issues closed on time), DELAYED ($\geq$70\% closed but $>$3 days late), FAILED ($<$70\% closed or abandoned)
    \item \textit{Blocker presence} (binary): Identified via explicit "blocked" labels or issues stagnant $>$5 days
    \item \textit{Blocker types} (multi-label): Technical, dependency, resource, requirement, external (labeled via keyword matching and manual verification on 500-sprint subset)
\end{itemize}

\textbf{Known Challenges:}
\begin{itemize}
    \item \textit{Class imbalance:} SUCCESS outcomes dominate ($\sim$60\%), requiring stratified sampling and class-weighted loss functions
    \item \textit{Label noise:} Automated labeling achieves $\sim$85\% accuracy; we manually verify 500 instances for evaluation reliability
    \item \textit{Temporal dependencies:} Sequential nature of sprint events requires careful temporal splitting to prevent data leakage
\end{itemize}

\subsubsection{Synthetic Dataset Generation}

\textbf{Purpose:} Enable cold-start deployment and augment rare failure scenarios.

\textbf{Generation Method:} We employ GPT-4 \cite{openai2023gpt4} to generate realistic sprint scenarios using structured prompts specifying team size, sprint duration, outcome target, and blocker types. Each synthetic sprint includes simulated commit patterns, issue workflows, and PR activities matching statistical distributions observed in real data.

\textbf{Size:} 5,000 synthetic scenarios (30\% edge cases, 50\% typical patterns, 20\% cross-repository complexity)

\textbf{Quality Validation:}
\begin{itemize}
    \item Statistical similarity: KL divergence $<$0.15 vs. real data distributions
    \item Expert review: 100 scenarios rated by software engineers (mean realism score $\geq$3.8/5.0)
    \item Discriminator test: Binary classifier distinguishing real vs. synthetic achieves only 58\% accuracy (near random baseline)
\end{itemize}

\subsubsection{Data Split Strategy}

\textbf{Temporal split:} Training on March 2023 - December 2025, validation on January 2026, testing on February 2026 prevents temporal leakage and mimics realistic future prediction.

\textbf{Cold-start evaluation:} Separate held-out organizations with only 1 week, 1 month, and 3 months history test synthetic augmentation effectiveness.

\subsection{Method: Technical Approach}

\subsubsection{System Architecture}

Our system employs a multi-agent architecture orchestrated through LangGraph with six specialized agents, as illustrated in Figure~\ref{fig:system_arch}:

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/1_system_architecture.png}}
\caption{Multi-Agent System Architecture showing the orchestration of six specialized agents (Data Collector, Feature Engineering, Sprint Analyzer, Risk Assessor, Recommender, and Explainer) via LangGraph. The ChromaDB Vector Store provides RAG capabilities for evidence-backed explanations.}
\label{fig:system_arch}
\end{figure}

\textbf{1. Data Collector Agent:} Interfaces with GitHub GraphQL API to fetch repository events, normalize data schemas across repositories, and detect sprint boundaries from milestone dates or 2-week rolling windows.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/2_data_pipeline.png}}
\caption{Data Pipeline and Feature Engineering Flow, illustrating the transformation of raw GitHub events into 120 multi-modal features across code, text, temporal, graph, sentiment, and CI/CD domains.}
\label{fig:pipeline}
\end{figure}

\textbf{2. Feature Engineering Agent:} Extracts 120 handcrafted features across six modalities:
\begin{itemize}
    \item Code: 25 features (lines changed, file churn, cyclomatic complexity, test coverage)
    \item Text: 25 features (TF-IDF vectors of issue/PR descriptions, sentiment scores extracted from developer communications \cite{li2022automating})
    \item Temporal: 20 features (burndown slope, velocity trends, activity rhythm)
    \item Graph: 20 features (dependency graph depth, contributor centrality, cross-repo edge count)
    \item Sentiment: 15 features (positive/negative ratios, urgency indicators, confusion signals)
    \item CI/CD: 15 features (test pass rates, build duration, deployment frequency)
\end{itemize}

\textbf{3. Sprint Analyzer Agent:} Uses Llama-2-7B \cite{touvron2023llama} (quantized to 4-bit, $\sim$5GB RAM) to identify patterns in feature vectors, comparing current sprint against historical distributions. Generates sprint health score (0-100) and anomaly flags.

\textbf{4. Risk Assessor Agent:} Predicts sprint outcomes and blocker presence using LLM reasoning augmented with retrieved historical cases. Queries ChromaDB vector store to find top-5 similar past sprints, providing context for prediction.

\textbf{5. Recommender Agent:} Generates ranked intervention strategies based on detected risks. Recommendations prioritize actionability, specificity, and timing considerations (e.g., "Schedule PR review for \#89 within 24 hours to unblock issues \#6 and \#7").

\textbf{6. Explainer Agent:} Produces natural language explanations with evidence citations. Retrieves relevant commits, issues, and PRs from vector store, constructs chain-of-thought reasoning, and formats output with hyperlinked evidence.

The complete system deployment architecture is shown in Figure~\ref{fig:deployment}, illustrating how all components are containerized using Docker Compose to run locally on a standard developer laptop.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/8_system_deployment.png}}
\caption{System Deployment Architecture using Docker Compose. The entire stack (Frontend, Backend, LLM, Database, Vector Store) is containerized to run locally on a standard developer laptop.}
\label{fig:deployment}
\end{figure}

\subsubsection{Training Strategy}

\textbf{Base Model:} Llama-2-7B-Instruct \cite{touvron2023llama} provides strong instruction-following and code understanding capabilities while fitting in 16GB RAM when quantized.

\textbf{Parameter-Efficient Fine-Tuning:} Rather than full fine-tuning (computationally prohibitive), we employ LoRA (Low-Rank Adaptation) \cite{hu2021lora} injecting trainable adapter layers with rank $r=16$, updating only 0.3\% of parameters ($\sim$21M out of 7B). This enables project-specific adaptation on consumer hardware. Figure~\ref{fig:lora} illustrates the LoRA architecture, comparing traditional fine-tuning with our parameter-efficient approach.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/7_lora_architecture.png}}
\caption{Parameter-Efficient LoRA Fine-Tuning Architecture. By updating only 0.3\% of parameters, LoRA enables the 7B model to be fine-tuned and deployed on consumer-grade laptops (16GB RAM).}
\label{fig:lora}
\end{figure}

\textbf{Training Data:} Combine 70\% synthetic scenarios with 30\% real organizational data during LoRA fine-tuning. Synthetic data provides diverse failure modes; real data captures authentic patterns.

\textbf{Loss Function:} Multi-task learning optimizing three objectives simultaneously:
\begin{equation}
\mathcal{L} = \alpha \mathcal{L}_{\text{outcome}} + \beta \mathcal{L}_{\text{blocker}} + \gamma \mathcal{L}_{\text{explanation}}
\end{equation}
where $\mathcal{L}_{\text{outcome}}$ is cross-entropy for 3-class outcome prediction, $\mathcal{L}_{\text{blocker}}$ is binary cross-entropy for blocker detection, and $\mathcal{L}_{\text{explanation}}$ is perplexity for explanation generation. Weights $\alpha=0.5, \beta=0.3, \gamma=0.2$ balance tasks.

\textbf{Hyperparameters:} Learning rate $3\times10^{-4}$, batch size 8 (gradient accumulation 4 steps), 3 epochs, cosine annealing schedule, AdamW optimizer.

\textbf{RAG Implementation:} Embed all historical sprint cases using Sentence-BERT (all-MiniLM-L6-v2, 384 dimensions) and store in ChromaDB. At inference, embed current sprint query, retrieve top-5 similar cases ($\geq$0.70 cosine similarity), and inject into context window.

\subsection{Evaluation Plan}

Our evaluation follows a rigorous 7-step methodology illustrated in Figure~\ref{fig:eval_method}, encompassing dataset preparation, baseline and proposed system training, and comprehensive testing across multiple dimensions.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.85\linewidth]{figures/4_evaluation_methodology.png}}
\caption{Evaluation Methodology Flowchart showing the 7-step evaluation protocol: (1) Dataset Preparation, (2) Baseline Training, (3) Proposed System Training, (4) Predictive Accuracy Testing, (5) Recommendation Quality Testing, (6) Explainability Evaluation, and (7) System Performance Testing.}
\label{fig:eval_method}
\end{figure}

\subsubsection{Metrics}

We employ multiple metric categories assessing different system aspects:

\textbf{Predictive Performance:}
\begin{itemize}
    \item Sprint outcome classification: Macro F1-score (primary), per-class precision/recall/F1
    \item Blocker detection: Binary F1-score, AUROC
    \item Calibration: Brier score, Expected Calibration Error
    \item Early warning: Mean lead time (days before deadline when risk flagged)
\end{itemize}

\textbf{Recommendation Quality:}
\begin{itemize}
    \item Ranking: NDCG@5, Mean Reciprocal Rank
    \item Relevance: Human ratings on 1-5 Likert scale
    \item Acceptance: Percentage of recommendations participants would act upon
\end{itemize}

\textbf{Explanation Quality:}
\begin{itemize}
    \item Trust: 5-item Likert scale survey (target $\geq$4.0/5.0) measuring user confidence in AI-generated insights \cite{ribeiro2016should}
    \item Evidence correctness: Citation accuracy (URL validity), relevance (supports claim)
    \item Fluency: BLEU and BERTScore vs. human-written explanations
\end{itemize}

\textbf{Systems Performance:}
\begin{itemize}
    \item Latency: p50, p95, p99 end-to-end analysis time (target p95 $<$60s)
    \item Resources: Peak RAM usage (target $\leq$14GB), CPU utilization
    \item Throughput: Analyses per hour under load
\end{itemize}

Figure~\ref{fig:evaluation} presents a comprehensive comparison of our proposed multi-agent system against baseline methods across all four evaluation dimensions.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/6_evaluation_framework.png}}
\caption{Evaluation Framework Comparison. Our proposed multi-agent system is compared against baseline methods (Rule-Based, XGBoost, Single LLM) across predictive accuracy, recommendation quality, explainability, and system performance.}
\label{fig:evaluation}
\end{figure}

\subsubsection{Validation Strategy}

\textbf{Temporal split:} Primary evaluation uses strict temporal hold-out (training on 2023-2025, testing on Feb 2026) preventing data leakage.

\textbf{Leave-one-organization-out:} Cross-validation across 50 organizations quantifies generalization to entirely new startups (zero-shot setting).

\textbf{Cold-start simulation:} Test performance with only 1-week, 1-month, and 3-month organizational histories, comparing synthetic augmentation ratios (0\%, 30\%, 50\%, 70\%, 90\%).

\textbf{Human evaluation:} 10 startup practitioners (tech leads, engineering managers) review 6 sprint scenarios, rating recommendations and explanations. Within-subjects design compares our system against baselines. Semi-structured interviews capture qualitative feedback.

\subsubsection{Statistical Testing}

Paired t-tests compare our approach against baselines on matched sprint instances. Bonferroni correction controls family-wise error rate across multiple comparisons ($\alpha=0.05/4=0.0125$). Report effect sizes (Cohen's d) alongside p-values. Bootstrap 95\% confidence intervals quantify uncertainty.

\subsection{Competing Methods}

We compare against four baselines demonstrating improvements over existing approaches:

\textbf{Baseline 1: Rule-Based Heuristics.} Traditional project management rules using velocity thresholds, burndown slope criteria, and issue closure ratios. Represents current manual practice. Target F1 $\sim$0.62-0.68.

\textbf{Baseline 2: Gradient Boosting (XGBoost).} State-of-the-art classical ML using 120 handcrafted features without LLM reasoning. Achieves strong performance in related work (F1 $\sim$0.72-0.78) \cite{choetkiertikul2018predicting}. Demonstrates value-add of semantic understanding.

\textbf{Baseline 3: Single-Prompt LLM.} Llama-2-7B with single comprehensive prompt, no multi-agent orchestration or RAG. Tests whether agent architecture provides benefits beyond vanilla LLM usage. Target F1 $\sim$0.75-0.80.

\textbf{Baseline 4: Multi-Agent without RAG.} Identical agent architecture but removes retrieval mechanism, testing RAG contribution to explanation trust and accuracy. Target trust score $\sim$2.8/5.0 (compared to 4.2/5.0 target with RAG).

\textbf{Justification:} These baselines span simple (rules), traditional ML (XGBoost), modern AI (single LLM), and near-variants (no RAG), enabling isolation of each contribution. Comparisons are appropriate because all address sprint outcome prediction using GitHub data, matching our problem domain.

\section{Current Progress and Timeline}

\subsection{Work Completed to Date}

As of February 15, 2026, the team has completed foundational research and planning activities:

\textbf{Completed Milestone 1: Literature Review and Topic Identification}
\begin{itemize}
    \item Conducted comprehensive literature analysis across GitHub analytics, LLM applications in software engineering, sprint prediction methods, retrieval-augmented generation, and parameter-efficient fine-tuning
    \item Reviewed 50+ peer-reviewed papers spanning 2009-2024 from ACM, IEEE, and arXiv repositories
    \item Identified three critical gaps in current practice: (1) over-engineered tools unsuitable for small teams, (2) limited cross-repository intelligence, and (3) lack of explainable AI for project management
    \item Documented innovation opportunities in multi-agent architectures, RAG-enhanced explainability, and synthetic data bootstrapping
    \item Formalized research objectives and hypothesis statements
    \item Developed comprehensive research proposal with detailed methodology and evaluation framework
\end{itemize}

\textbf{Evidence of Progress:}
\begin{itemize}
    \item Research objectives document completed (January 15, 2026)
    \item System architecture design specifications drafted
    \item Evaluation framework methodology documented
    \item Project proposal submitted for course approval (February 15, 2026)
    \item GitHub repository initialized: \url{https://github.com/bibekgupta3333/repo-sprint}
\end{itemize}

\textbf{Next Steps:} Beginning Week 2 (February 24), the team will commence implementation starting with dataset collection infrastructure as detailed in the timeline below.

\subsection{Implementation Schedule}

\begin{table*}[htbp]
\centering
\caption{Project Timeline with Milestones and Team Assignments}
\label{tab:timeline}
\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm}|p{4.5cm}|p{2.5cm}|p{4cm}}
\toprule
\textbf{Week} & \textbf{Milestone} & \textbf{Team Member} & \textbf{Deliverables} \\
\midrule
Week 1 & \textbf{M1: Literature Review} & All members & Comprehensive literature analysis; gap identification \\
Feb 17-23 & Research paper analysis, documentation & & Research objectives report \\
\midrule
Week 2 & \textbf{M2: Dataset Collection} & Saarupya & GitHub Archive downloader; 9,000 sprint instances \\
Feb 24-Mar 2 & Data pipeline implementation & & Labeled dataset, data statistics report \\
\midrule
Week 3 & \textbf{M3: Synthetic Data Generation} & Siwani & GPT-4 based scenario generation; 5,000 synthetic sprints \\
Mar 3-9 & Quality validation, realism testing & & Synthetic dataset, validation metrics \\
\midrule
Week 4 & \textbf{M4: Feature Engineering} & Saarupya & 120-feature extraction pipeline functional \\
Mar 10-16 & Multi-modal feature extraction & & Feature engineering module, documentation \\
\midrule
Week 5 & \textbf{M5: Baseline Implementation} & Siwani & Rule-based, XGBoost, Single-LLM baselines \\
Mar 17-23 & Training and evaluation & & Baseline results, performance benchmarks \\
\midrule
Week 6 & \textbf{M6: System Architecture} & Bibek, Saarupya & 6-agent architecture designed and documented \\
Mar 24-30 & LangGraph orchestration setup & & Architecture diagrams, design specifications \\
\midrule
Week 7 & \textbf{M7: LoRA Fine-Tuning} & Siwani & LoRA adapters trained; validation F1 $\geq$0.82 \\
Mar 31-Apr 6 & Hyperparameter tuning & & Model checkpoints, training logs \\
\midrule
Week 8 & \textbf{M8: RAG Implementation} & Bibek & ChromaDB integration; evidence retrieval working \\
Apr 7-13 & Embedding generation, retrieval optimization & & RAG pipeline, retrieval metrics \\
\midrule
Week 9 & \textbf{M9: Multi-Agent Integration} & Bibek, Saarupya & End-to-end 6-agent pipeline functional \\
Apr 14-20 & Agent orchestration, testing & & Integrated system, unit tests \\
\midrule
Week 10 & \textbf{M10: Evaluation Framework} & Siwani, Deepthi & Automated evaluation; metrics calculation \\
Apr 21-27 & Statistical testing, human evaluation & & Evaluation results, comparison analysis \\
\midrule
Week 11 & \textbf{M11: Dashboard Development} & Deepthi & Interactive web dashboard deployed \\
Apr 28-May 4 & Streamlit frontend, visualization & & Functional prototype, user interface \\
\midrule
Week 12 & \textbf{M12: Systems Optimization} & Saarupya, Bibek & Latency $<$60s p95; RAM $<$14GB peak \\
May 5-11 & Performance profiling, optimization & & Benchmark results, optimization report \\
\midrule
Week 13 & \textbf{M13: Documentation \& Report} & All members & Complete technical documentation \\
May 12-18 & Writing, editing, final testing & & Academic paper, GitHub repository \\
\midrule
Week 14 & \textbf{M14: Presentation} & All members & Project presentation and demo \\
May 19-25 & Slide deck, demo preparation & & 20-minute presentation with live demonstration \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Risk Mitigation}

\textbf{Risk 1: LoRA fine-tuning underfits due to limited real data.}
\textit{Mitigation:} Increase synthetic data ratio to 80\%; validate performance on held-out real test set; if insufficient, supplement with active learning selecting most informative real examples for manual labeling.

\textbf{Risk 2: Latency exceeds 60-second target on 16GB hardware.}
\textit{Mitigation:} Implement caching for repeated queries; parallelize feature extraction; reduce RAG top-k from 5 to 3; profile and optimize bottleneck stages; document minimum hardware requirements if optimization insufficient.

\textbf{Risk 3: Human evaluation recruitment challenges.}
\textit{Mitigation:} Leverage startup accelerator networks and open-source communities; offer \$100 compensation for 90-minute sessions; prepare asynchronous evaluation options allowing remote participation.

\textbf{Risk 4: Baseline outperforms proposed system.}
\textit{Mitigation:} Conduct thorough error analysis identifying failure modes; iterate on agent prompts and retrieval strategies; report honestly and position as exploratory study identifying when multi-agent approaches help vs. hinder; consider this a valuable negative result.

\section{Target Outcomes and Expected Impact}

Upon successful completion, this project is expected to deliver:

\textbf{Academic Contributions:}
\begin{itemize}
    \item First empirical evaluation of multi-agent LLM systems for cross-repository sprint analysis
    \item Novel synthetic data generation methodology for software project management
    \item Comprehensive evaluation framework measuring predictive quality, explainability, and operational feasibility
    \item Insights into parameter-efficient fine-tuning effectiveness for project-specific adaptation
\end{itemize}

\textbf{Practical Deliverables:}
\begin{itemize}
    \item Open-source system deployable on standard developer laptops
    \item Publicly released dataset and evaluation protocols
    \item Documentation enabling practitioners to adapt system for their contexts
    \item Interactive dashboard prototype demonstrating sprint health visualization
\end{itemize}

\textbf{Broader Impact:} This work democratizes advanced sprint intelligence for resource-constrained startups, reducing manual tracking overhead, enabling data-driven project decisions, and increasing development velocity. By proving local deployment viability, we address privacy concerns preventing adoption of cloud-based analytics. The evaluation methodology establishes standards for assessing LLM systems in software engineering domains, benefiting future research.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
